
* Change cache directory
https://huggingface.co/docs/transformers/installation#cache-setup
https://stackoverflow.com/questions/61798573/where-does-hugging-faces-transformers-save-models

The default path of cache directory is
- "~/.cache/huggingface/hub" as of transformers 4.22
- "~/.cache/torch/transformers" otherwise

If you want to change it, specify the path with =HUGGINGFACE_HUB_CACHE= or =TRANSFORMERS_CACHE=.

e.g. Running python code with a specified cache location
#+begin_src sh
# absolute path
HUGGINGFACE_HUB_CACHE=./some/path python code.py
# TRANSFORMERS_CACHE=./some/path python code.py

# path with user's home
HUGGINGFACE_HUB_CACHE=~/some/path python code.py
# TRANSFORMERS_CACHE=~/some/path python code.py

# export the path
export HUGGINGFACE_HUB_CACHE=some/path
python code.py
#+end_src

* Downloading a model
https://huggingface.co/docs/hub/models-downloading

#+begin_src sh
git lfs install

MODEL_NAME=facebook/bart-base
MODEL_URL=https://huggingface.co/$MODEL_NAME
git clone git clone $MODEL_URL
#+end_src

* Auto classes
https://huggingface.co/docs/transformers/model_doc/auto#auto-classes
https://huggingface.co/docs/transformers/autoclass_tutorial
- AutoTokenizer
- AutoModel
  - AutoModelForSeq2SeqLM
  - AutoModelForSequenceClassification
  - etc
- AutoConfig
- etc
** AutoProcessor
https://huggingface.co/docs/transformers/preprocessing

- AutoProcessor is more general than AutoTokenizer or AutoFeatureExtractor
- AutoProcessor also can be used to preprocess multi-modal data
- e.g.
  #+begin_src python
  type(AutoProcessor.from_pretrained("bert-base-cased")) is type(AutoTokenizer.from_pretrained("bert-base-cased"))
  #+end_src

* Tokenizer
** tokenize and decode
#+begin_src python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer.tokenize("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
encoded = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
tokenizer.decode(encoded["input_ids"])
#+end_src
*** decoding without special tokens
Use ~skip_special_tokens~ option
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer("Hello World")
# {'input_ids': [0, 725, 33796, 623, 2], 'attention_mask': [1, 1, 1, 1, 1]}
print(tokenizer.decode(encoded['input_ids']))
# '<s>Hello World</s>'
print(tokenizer.decode(encoded['input_ids'], skip_special_tokens=True))
# 'Hello World'
#+end_src
** "add_prefix_space" options
RobertaTokenizer (and its subclass BartTokenizer) has ~add_prefix_space~ parameter for ~__init__~ and ~__call__~.
The option add an additional space character to the beginning of the input sentence.

- Example 1
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer1 = RobertaTokenizer.from_pretrained("roberta-base")
  print(tokenizer1("Hello world"))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1("Hello world", add_prefix_space=True))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer1(" Hello world")) # input text includes a space character at the beginning
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
- Example 2
  #+begin_src python
  from transformers import RobertaTokenizer
  tokenizer2 = RobertaTokenizer.from_pretrained("roberta-base", add_prefix_space=True)
  print(tokenizer2("Hello world"))
  # {'input_ids': [0, 20920, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  print(tokenizer2("Hello world", add_prefix_space=False))
  # {'input_ids': [0, 31414, 232, 2], 'attention_mask': [1, 1, 1, 1]}
  #+end_src
** tensors as results
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer(['Hello World!', 'My mother likes flowers.'], padding=True, return_tensors="pt")
#+end_src
** PreTrainedTokenizer.convert_ids_to_tokens
it maps token ids to tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
encoded = tokenizer('Hello World!')
print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))
# ['<s>', 'Hello', 'ĠWorld', '!', '</s>']
#+end_src
** convert functions
#+begin_src python
tokenizer.convert_ids_to_tokens([1,2])
# ['<pad>', '</s>']
tokenizer.convert_ids_to_tokens([1,2,-1])
# ['<pad>', '</s>', None]
tokenizer.convert_ids_to_tokens([1,2,100000000000000])
# ['<pad>', '</s>', None]
tokenizer.convert_ids_to_tokens([1,2])
# ['<pad>', '</s>']
tokenizer.convert_ids_to_tokens(1)
# '<pad>'
tokenizer.convert_tokens_to_ids(['<pad>', '</s>'])
# [1, 2]
tokenizer.convert_tokens_to_ids('<pad>')
# 1
tokenizer.convert_tokens_to_string(['<pad>', '</s>'])
# '<pad></s>'
#+end_src
** batch-decoding
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")

encoded = tokenizer(['I like an apple.', 'A bird flies.'])
# {'input_ids': [[0, 100, 101, 41, 15162, 4, 2], [0, 250, 5103, 16016, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
utteances = tokenizer.batch_decode(encoded['input_ids'], skip_special_tokens=True)
# ['I like an apple.', 'A bird flies.']
#+end_src
** tokenizer example
https://huggingface.co/transformers/v3.0.2/preprocessing.html
** iterate tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
all_token_values = tuple(map(tokenizer.convert_ids_to_tokens, range(tokenizer.vocab_size)))
#+end_src
*** iterate non-special tokens
#+begin_src python
from transformers import BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
all_special_ids = set(tokenizer.all_special_ids)
all_non_special_tokens = tuple(
    tokenizer.convert_ids_to_tokens(token_id)
    for token_id in range(tokenizer.vocab_size)
    if token_id not in all_special_ids)
return all_non_special_tokens
#+end_src
** GPT2Tokenizer
- BartTokenizer is a subclass of GPT2Tokenizer
*** ~_convert_id_to_token~
~GPT2Tokenizer~ has has attributes named as ~encoder~ and ~decoder~ which are dictionaries
that map tokens to ids and ids to tokens respectively.
~GPT2Tokenizer._convert_id_to_token~ exploits ~encoder~ and ~decoder~.
** T5Tokenizer example
#+begin_src python
from transformers import T5Tokenizer, T5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained("google-t5/t5-small")
tokenizer.tokenize("I like an apple and want to eat it.")
# ['▁I', '▁like', '▁an', '▁apple', '▁and', '▁want', '▁to', '▁', 'eat', '▁it', '.']
#+end_src

** tokenizer.vocab_size vs. len(tokenizer)
#+begin_src python
from transformers import BartTokenizer

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
assert len(tokenizer) == tokenizer.vocab_size
initial_num_tokens = len(tokenizer)

new_tokens = ['<special-1>', '<special-2>']
added_tokens_num = tokenizer.add_tokens(new_tokens, special_tokens=True)

assert len(tokenizer) == tokenizer.vocab_size + added_tokens_num
#+end_src


* PreTrainedModel
** resize_token_embeddings
e.g.
#+begin_src python
from transformers import BartTokenizer

tokenizer = BartTokenizer.from_pretrained("facebook/bart-base")
new_tokens = ['<special-1>', '<special-2>']
added_tokens_num = tokenizer.add_tokens(new_tokens, special_tokens=True)
model.resize_token_embeddings(len(tokenizer))
#+end_src

* Examples
** running with scripts
https://huggingface.co/docs/transformers/run_scripts
https://github.com/huggingface/transformers/tree/main/examples/pytorch
** run a model after a tokenizer
#+begin_src python
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
pt_outputs = pt_model(**pt_batch)
#+end_src
** run a pipeline with a model and a tokenizer
#+begin_src python
from transformers import pipeline
generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
#+end_src

* Accelerate
** Install
#+begin_src sh
pip install accelerate
#+end_src

** Make a config file
#+begin_src sh
CONFIG_FILE=path/to/config.yaml
mkdir -p $(dirname $CONFIG_FILE)
accelerate config --config_file $CONFIG_FILE
#+end_src

** Accelerator
*** ~__init__~
**** split_batches (default = False)
- When it's True, the size of batch of a data loader is divided by the number of processes.
  Therefore, the number of examples in batches in all processes is the batch size of the data loader.
  (In other words, The batch size of each process is 1/#processes of the batch size of the data loader.)
- When it's False, the batch of each process has the same batch size of the data loader.
**** even_batches (default = True)
- When it's True, each process has exactly the same number of batches for each epoch.
  To make the even number of batches, some processes have batches that include examples that are already used in some processes.
- When it's False, each process can have different numbers of batches.
  The difference the numbers of batches between two processes is up to 1 batch.
  However, the difference can raises a deadlock condition when computing Accelerator.backward.

** split_between_processes
The function "split_between_processes" is included in Accelerator, AcceleratorState or PartialState

#+begin_src python
# Usage:
# $ accelerate launch --num_processes 2 example.py
#
# Prerequisites:
# - To use N processes, CUDA_VISIBLE_DEVICES should contain N GPU numbers

# Assume there are two processes
from accelerate import PartialState

state = PartialState()
with state.split_between_processes(["A", "B", "C"]) as inputs:
    print(inputs)

# # Process 0
# ["A", "B"]
# # Process 1
# ["C"]

with state.split_between_processes(["D", "E", "F"], apply_padding=True) as inputs:
    print(inputs)

# # Process 0
# ["D", "E"]
# # Process 1
# ["F", "F"]
#+end_src

** AcceleratedOptimizer
by default, AcceleratedOptimizer adds the number of processes to self.optimizer.last_epoch for each synchronized epoch.

** accelerate.data_loader.prepare_data_loader
~prepare_data_loader~ returns a replacement of ~torch.utils.data.DataLoader~.
For example, ~DataLoaderShard~ can replace ~DataLoader~.
~prepare_data_loader~ calls ~DataLoaderShard.__init__~ which takes ~BatchSamplerShard~ object as the argument for the ~batch_sampler~ parameter for ~DataLoader~, since ~DataLoader~ is the super class of ~DataLoaderShard~.
The ~BatchSamplerShard.__iter__~ yields indices of examples for batches depending on the ~process_index~.
Therefore, each process receives different batches of examples.

** process_index
=Accelerator= and =PartialState= has =process_index= as an property,
which can be considered as an identifier.
=process_index= ranges from 0 to (N - 1) where N is the number of processes.

** Examples
*** Accelerator.backward
Accelerator.backward assumes that loss values on a batch is averaged rather than summed.

#+begin_src python
# Usage:
# CUDA_VISIBLE_DEVICES=0,1 accelerate launch --num_processes 2 accelerate_backward.py

from accelerate import Accelerator
import torch


accelerator = Accelerator(split_batches=True, even_batches=False)

class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.params = torch.nn.Parameter(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))

    def forward(self, batch):
        # Note: compute the mean of loss values from examples rather than sum of the values

        # loss = (self.params * batch).sum(dim=1).sum(dim=0)
        loss = (self.params * batch).sum(dim=1).mean()
        return loss


model = Model()

tensor = torch.tensor([2, 2, 2, 2, 2], dtype=torch.float)
data_loader = torch.utils.data.DataLoader([tensor] * 20, batch_size=4, shuffle=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1.0)

model, data_loader, optimizer = accelerator.prepare(model, data_loader, optimizer)

for batch in data_loader:
    loss = model(batch)
    print('Batch-size: ', batch.shape[0])
    print('Loss: ', loss)

    optimizer.zero_grad()
    accelerator.backward(loss)
    # loss.backward()

    print('Gradients: ', tuple(model.parameters())[0].grad)

    optimizer.step()
    print('Parameters: ', tuple(model.parameters()))

    break
#+end_src

*** split_batches
In the following example, the value of split_batches doesn't change the result.

#+begin_src python
# CUDA_VISIBLE_DEVICES=0,1 accelerate launch --num_processes 2 accelerate_batch.py

from accelerate import Accelerator
import torch


SPLIT_BATCHES = False
# SPLIT_BATCHES = True
accelerator = Accelerator(split_batches=SPLIT_BATCHES, even_batches=False)
print = accelerator.print
BATCH_SIZE = accelerator.num_processes * 4 if SPLIT_BATCHES else 4

print('split-batches: ', SPLIT_BATCHES)
print('batch size: ', BATCH_SIZE)


class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.params = torch.nn.Parameter(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))

    def forward(self, batch):
        # Note: compute the mean of loss values from examples rather than sum of the values

        # loss = (self.params * batch).sum(dim=1).sum(dim=0)
        loss = (self.params * batch).sum(dim=1).mean()
        return loss


model = Model()

tensor = torch.tensor([2, 2, 2, 2, 2], dtype=torch.float)
data_loader = torch.utils.data.DataLoader([tensor] * 32, batch_size=BATCH_SIZE, shuffle=False)
optimizer = torch.optim.SGD(model.parameters(), lr=1.0)

class PrintingMultiplicativeLR(torch.optim.lr_scheduler.MultiplicativeLR):
    def get_lr(self):
        return_value = super().get_lr()
        print('get_lr :', return_value)
        return return_value


scheduler = PrintingMultiplicativeLR(optimizer, lambda epoch: 0.9)

model, data_loader, optimizer = accelerator.prepare(model, data_loader, optimizer)

for batch in data_loader:
    loss = model(batch)
    # print('Batch-size: ', batch.shape[0])
    # print('Loss: ', loss)

    optimizer.zero_grad()
    accelerator.backward(loss)

    # print('Gradients: ', tuple(model.parameters())[0].grad)

    optimizer.step()
    scheduler.step()
    print('lr: ', scheduler._last_lr)
    # print('Parameters: ', tuple(model.parameters()))

    # break

print('Parameters: ', tuple(model.parameters()))
#+end_src

* Error fix
** Accelerator
*** RuntimeError: NCCL communicator was aborted on rank 2
**** Error message
#+begin_example
RuntimeError: NCCL communicator was aborted on rank 2.  Original reason for failure was: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=9, OpType=ALLREDUCE, Timeout(ms)=1800000) ran for 1801740 milliseconds before timing out.
[E ProcessGroupNCCL.cpp:414] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
#+end_example

The error occured when ~accelerate.utils.operations.gather_object~ is called.

**** Solution
https://github.com/huggingface/accelerate/issues/314#issuecomment-1565259831
https://stackoverflow.com/a/69695109

#+begin_src bash
export NCCL_P2P_LEVEL=NVL
#+end_src
